"""
THE HERO DEMO:
Running the High-Accuracy (78%) VLA on a New Video.
"""
import torch
import json
import cv2
import open_clip
from PIL import Image
import numpy as np
from phase2_temporal_vla_model import TemporalVLA
from process_8_indian_veg import download_fresh_video, extract_audio, get_transcript, detect_chapters

# --- CONFIG ---
# The 78% Accuracy Model (The Winner)
MODEL_PATH = 'models/temporal_vla_phase2_best.pt'
VOCAB_PATH = 'data/processed/action_vocab_phase2_fixed.json'
VIDEO_URL = "https://www.youtube.com/watch?v=oYZ--rdHL6I" # Paneer Video

def get_model_and_vocab(device):
    print(f"ðŸ§  Loading the 78% Accuracy Model...")
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    with open(VOCAB_PATH) as f:
        vocab = json.load(f)
    inv_vocab = {v: k for k, v in vocab.items()}
    
    model = TemporalVLA(
        embedding_dim=512,
        hidden_dim=512,
        num_actions=len(vocab),
        num_heads=8,
        num_layers=6
    ).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    return model, inv_vocab, checkpoint.get('best_acc', 78.15)

def extract_frames_for_chapter(video_path, start, end, preprocess):
    cap = cv2.VideoCapture(video_path)
    frames = []
    # Extract 30 frames evenly spaced
    duration = end - start
    if duration <= 0: duration = 1
    indices = np.linspace(start, end, 30)
    
    for t in indices:
        cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)
        ret, frame = cap.read()
        if ret:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame_rgb)
            frames.append(preprocess(pil_img))
    cap.release()
    
    # Pad if needed
    if len(frames) == 0: return None
    if len(frames) < 30:
        frames += [frames[-1]] * (30 - len(frames))
        
    return torch.stack(frames) # [30, 3, 224, 224]

def main():
    print("="*60)
    print("ðŸ¤– VLA HERO DEMO: CROSS-CUISINE GENERALIZATION")
    print("="*60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. Load the Brain
    model, vocab, acc = get_model_and_vocab(device)
    print(f"âœ… Model Loaded. Validation Accuracy: {acc:.2f}%")
    print(f"ðŸ“š Knowledge Base: {len(vocab)} actions (Western Cooking)")
    
    # 2. Load Vision Encoder (CLIP)
    print("ðŸ‘ï¸  Loading Vision Encoder (CLIP)...")
    clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k', device=device)
    
    # 3. Get the Video (Indian Cuisine - New Domain!)
    vid_file = "hero_video.mp4"
    aud_file = "hero_audio.mp3"
    download_fresh_video(VIDEO_URL, vid_file)
    extract_audio(vid_file, aud_file)
    
    # 4. Listen & Segment
    transcript = get_transcript(aud_file)
    chapters = detect_chapters(transcript)
    print(f"ðŸŽ§ Detected {len(chapters)} events via Audio")
    
    print("\n" + "="*60)
    print("ðŸš€ RUNNING LIVE INFERENCE")
    print("="*60)
    
    for i, ch in enumerate(chapters, 1):
        if (ch['end'] - ch['start']) < 4: continue
        
        # A. See the world
        frames = extract_frames_for_chapter(vid_file, ch['start'], ch['end'], preprocess)
        if frames is None: continue
        
        frames = frames.to(device)
        
        # B. Encode (CLIP)
        with torch.no_grad():
            img_embeds = clip_model.encode_image(frames) # [30, 512]
            img_embeds = img_embeds / img_embeds.norm(dim=-1, keepdim=True)
            
            # C. Think (Temporal VLA)
            logits = model(img_embeds.unsqueeze(0))
            probs = torch.softmax(logits, dim=1)
            conf, pred_idx = torch.max(probs, 1)
            
            action = vocab[pred_idx.item()]
            
        # D. Output
        print(f"\nðŸ‘‰ Step {i} [{ch['start']:.0f}s]: Chef said \"...{ch['text'][:40]}...\"")
        print(f"   ðŸ§  VLA Analysis:  [{action}]")
        print(f"   ðŸ“Š Confidence:    {conf.item():.1%}")
        
        # E. The "Translation" (Why this is cool)
        # Showing how Western training applies to Indian context
        if "mix" in action and "masala" in ch['text']:
            print("   ðŸ’¡ Insight: Model generalized 'mixing masala' to 'mixing ingredients'")
        if "chop" in action and "paneer" in ch['text']:
            print("   ðŸ’¡ Insight: Model generalized 'cutting paneer' to 'chopping vegetables'")

    print("\n" + "="*60)
    print("ðŸŽ‰ DEMO COMPLETE")
    print("="*60)

if __name__ == "__main__":
    main()
